---
title: 'Example 1. Analyzing visually recorded cover-abundance data'
author: "A. Onofri, H-P. Piepho and Marcin Kozak"
output: 
  html_document:
    toc: yes
    toc_float: 
     collapsed: false
     smooth_scroll: false
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# The dataset

The dataset refers to a field experiment to compare weed control ability of nine post-emergence herbicides against *Sorghum halepense* in maize. Three weeks after the treatment, the cover-abundance of *S. halepense* was visually recorded in six classes, following the Braun-Blanquet method. The limits of the classes are shown as ‘L’ (lower limit) and ‘U’ (upper limit). The ‘midPoint’ represents the center of each class.

First of all, we need to read the data into R. As the dataset is contained in the companion package 'agriCensData', we need to load this package first, assuming that it has been already installed in the system (as shown [here](https://pages.github.com/) ) 

R code
The first step to accomplish the analyses is to read the dataset into R:

BBsurvey <- data.frame(
  Herbicide = factor(rep(LETTERS[1:9], each=4)), 
  L = c(0.1, 0.1, 5, 5, 0.1, 0.1, 5, 5, 0, 0.1, 5, 5, 0, 0.1,
       5, 5, 0, 0, 0.1, 0.1, 5, 5, 25, 25, 0.1, 0.1, 5, 5, 
       0, 0.1, 5, 5, 75, 25, 50, 25),
  U = c(5, 5, 25, 25, 5, 5, 25, 25, 0.1, 5, 25, 25, 0.1, 
       5, 25, 25, 0.1, 0.1, 5, 5, 25, 25, 50, 50, 5, 5, 
       25, 25, 0.1, 5, 25, 25, 100, 50, 75, 50),
  midPoint = c(2.55, 2.55, 15, 15, 2.55, 2.55, 15, 15, 0.05, 2.5, 15,
              15, 0.05, 2.55, 15, 15, 0.05, 0.05, 2.55, 2.55, 15, 15,
              37.5, 37.5, 2.5, 2.55, 15, 15, 0.1, 2.55, 15, 15, 
              87.5, 37.5, 62.5, 37.5) )

head(BBsurvey)
#   Herbicide   L  U midPoint
# 1         A 0.1  5     2.55
# 2         A 0.1  5     2.55
# 3         A 5.0 25    15.00
# 4         A 5.0 25    15.00
# 5         B 0.1  5     2.55
# 6         B 0.1  5     2.55

Once the dataset has been read, a traditional ANOVA model can be fit, by using the mid point for each class as the dependent variable. Means and multiple comparison testing may be obtained by using the emmeans package (Lenth, 2018)

library(emmeans)
mod.aov <- lm(midPoint ~ Herbicide, data = BBsurvey)
means <- emmeans(mod.aov, ~Herbicide) 
cld(means, Letter = LETTERS, sort=F)

#  Herbicide  emmean       SE df  lower.CL upper.CL .group
#  A          8.7750 5.501751 27 -2.513661 20.06366  A    
#  B          8.7750 5.501751 27 -2.513661 20.06366  A    
#  C          8.1375 5.501751 27 -3.151161 19.42616  A    
#  D          8.1500 5.501751 27 -3.138661 19.43866  A    
#  E          1.3000 5.501751 27 -9.988661 12.58866  A    
#  F         26.2500 5.501751 27 14.961339 37.53866  A    
#  G          8.7625 5.501751 27 -2.526161 20.05116  A    
#  H          8.1625 5.501751 27 -3.126161 19.45116  A    
#  I         56.2500 5.501751 27 44.961339 67.53866   B   
# 
# Confidence level used: 0.95 
# P value adjustment: tukey method for comparing a family of 9 estimates 
# significance level used: alpha = 0.05 

To fit a survival model, we use the limits of each class as the dependent variables, instead of ‘midPoint’. In order to fit this model, we need to load the survival package (Therneau, 2015).

library(survival)
mod.surv <- survreg(Surv(L, U, type="interval2") ~ Herbicide, 
  dist="gaussian", data = BBsurvey)
means.surv <- emmeans(mod.surv, ~Herbicide) 
cld(means.surv, Letters = LETTERS, sort=F)

#  Herbicide    emmean       SE df   lower.CL  upper.CL .group
#  A          6.755275 3.684287 26 -0.8178844 14.328434  AB   
#  B          6.755275 3.684287 26 -0.8178844 14.328434  AB   
#  C          5.903803 3.631445 26 -1.5607388 13.368345  A    
#  D          5.903803 3.631445 26 -1.5607388 13.368345  A    
#  E          1.269626 3.253208 26 -5.4174388  7.956691  A    
#  F         25.022493 4.008432 26 16.7830434 33.261942   B   
#  G          6.755275 3.684287 26 -0.8178844 14.328434  AB   
#  H          5.903803 3.631445 26 -1.5607388 13.368345  A    
#  I         57.335193 3.772500 26 49.5807077 65.089678    C  
# 
# Results are given on the Surv (not the response) scale. 
# Confidence level used: 0.95 
# P value adjustment: tukey method for comparing a family of 9 estimates 
# significance level used: alpha = 0.05 

SAS Code
Here, it would be nice to give the code to perform with SAS the very same analyses as before (read the data, make an ANOVA and fit a survival model). Or we simply skip this part
Example 2: Time-to-event data
This dataset refers to a germination assay at two temperature levels (15 and 25°C). Germinated seeds were counted and removed on 20 dates after the beginning of the assay.

R code
Also for this example, the first step consists of reading the dataset into R:

germination <- data.frame(
obsT = c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 
  33, 35, 37, 39, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 
  27, 29, 31, 33, 35, 37, 39),
cumProp = c(0.02, 0.14, 0.25, 0.38, 0.52, 0.56, 0.62, 0.66, 0.71, 0.76, 
  0.78, 0.79, 0.81, 0.82, 0.85, 0.87, 0.89, 0.91, 0.91, 0.92, 0.1, 
  0.36, 0.49, 0.58, 0.68, 0.71, 0.77, 0.81, 0.82, 0.84, 0.86, 0.89, 
  0.89, 0.9, 0.92, 0.94, 0.94, 0.94, 0.95, 0.96),
temp = factor( rep(c(15, 25), each=20) ) )

head(germination)
#   obsT cumProp temp
# 1    1    0.02   15
# 2    3    0.14   15
# 3    5    0.25   15
# 4    7    0.38   15
# 5    9    0.52   15
# 6   11    0.56   15

The traditional method of data analysis is based on:
(1) transforming the observed counts into cumulative proportions of germinated seeds, and
(2) fitting a Gaussian cumulative probability density function, using nonlinear least squares regression.

cumProp <- as.numeric(unlist(with(germination, 
                                  tapply(counts, temp, cumsum) ))/100)

mod <- drm(cumProp ~ obsT, fct=LN.2(), curveid=temp,
           pmodels=list(~1, ~temp-1), data = germination)
summary(mod)
# Model fitted: Log-normal with lower limit at 0 and upper limit at 1 (2 parms)
# 
# Parameter estimates:
# 
#               Estimate Std. Error t-value   p-value    
# b:(Intercept) 0.871778   0.017726  49.182 < 2.2e-16 ***
# e:temp15      9.106622   0.178919  50.898 < 2.2e-16 ***
# e:temp25      5.429718   0.136514  39.774 < 2.2e-16 ***
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# 
# Residual standard error:
# 
#  0.02043963 (37 degrees of freedom)

For this example, a survival regression model is more appropriate. The code does the following:
(1) The dataset is reorganised, so that there is one record for each seed in the lot (200 records in all). For each temperature subset, records are added also for the seeds that did not germinate at the end of the assay.
(2) Each record is characterised by three variables: the temperature at which it was tested and the two limits of the interval within which it germinated (‘timeBef’ and ‘timeAf’). Seeds which already germinated at the first monitoring time have ‘timeBef’ equal to NA (not available). Seeds which did not germinate at the final monitoring time have ‘timeAf’ equal to NA.
(3) A survival model is fitted using a lognormal distribution of germination times (dist = “lognormal”).


counts <- c(counts[1:20], 100 - sum(counts[1:20]), counts[21:40], 100 - sum(counts[21:40])) 

germination2 <- data.frame(
  temp = rep(factor(rep(c(15, 20), each=21)), counts),
  timeAf = rep(c(germination$obsT[1:20], NA, germination$obsT[21:40], NA), counts),
  timeBef = rep(c(NA, germination$obsT[1:20], NA, germination$obsT[21:40]), counts) )
rm(counts)
head(germination2, 10)

#    temp timeAf timeBef
# 1    15      1      NA
# 2    15      1      NA
# 3    15      3       1
# 4    15      3       1
# 5    15      3       1
# 6    15      3       1
# 7    15      3       1
# 8    15      3       1
# 9    15      3       1
# 10   15      3       1

library(survival)
library(emmeans)

mod2 <- survreg(Surv(timeBef, timeAf, type = "interval2") ~ temp,
  dist = "lognormal", data = germination2)
summary(mod2)

# Value Std. Error     z        p
# (Intercept)  2.241     0.1173 19.11 2.11e-81
# temp20      -0.622     0.1666 -3.73 1.89e-04
# Log(scale)   0.145     0.0563  2.58 9.89e-03
# 
# Scale= 1.16 
# 
# Log Normal distribution
# Loglik(model)= -525.3   Loglik(intercept only)= -532.1
# Chisq= 13.57 on 1 degrees of freedom, p= 0.00023 
# Number of Newton-Raphson Iterations: 3 
# n= 200 

emmeans(mod2, ~temp, transform="response")
#  temp response        SE  df lower.CL  upper.CL
#  15   9.407324 1.1034551 197 7.231223 11.583425
#  20   5.050110 0.5974451 197 3.871901  6.228319
# 
# Confidence level used: 0.95 

SAS code
Here as well, it would be nice to give the code to perform with SAS the very same analyses as before (read the data, make a nonlinear least squares regression and fit a survival regression model). . Or we simply skip this part

Example 3: potato starch grain assessed in classes

This dataset refers to an experiment which aimed to compare diameters of starch grains from tubers of two potato producers. Starch grains were sampled from tubers, collected from the production fields of the producers. The dataset shows the counts of starch grains assigned to one of five diameter classes (<4, 4-8, 8-12, 12-16, >16 µm). For each producer, the diameters were taken from twelve photos taken with a microscope.

R code
The original dataset is in gouped form and consists of the counts of starch grains assigned to each of 5 diameter classes, as observed in 24 photos:

Class <- matrix(c(21, 23, 17, 24, 19, 13, 34, 23, 21, 26, 29, 40, 37, 31, 23,
           32, 32, 22, 29, 28, 28, 33, 20, 34, 42, 45, 40, 42, 16, 7, 16,
           13, 28, 27, 25, 19, 33, 40, 37, 36, 47, 18, 20, 13, 12, 13, 13, 
           12, 40, 22, 33, 30, 15, 11, 8, 10, 26, 27, 21, 22, 30, 24, 31,
           30, 28, 25, 27, 21, 16, 24, 13, 8, 29, 12, 18, 8, 8, 4, 5, 4, 8,
           14, 16, 7, 27, 20, 13, 5, 20, 22, 14, 17, 13, 12, 20, 19, 14, 10,
           8, 3, 3, 10, 9, 1, 8, 8, 7, 10, 19, 15, 19, 20, 20, 13, 18, 11,
           21, 26, 22, 16), 24, 5)
colnames(Class) <- paste("c", 1:5, sep = "")
Group <- rep(c("P1", "P2"), each = 12)
Photo <- rep(1:24)
dataset <- data.frame(Group = Group, Photo = Photo, Class)
rm(Class, Group, Photo) 
head(dataset)
 #   Group Photo c1 c2 c3 c4 c5
 # 1    P1     1 21 42 40 29 14
 # 2    P1     2 23 45 22 12 10
 # 3    P1     3 17 40 33 18  8
 # 4    P1     4 24 42 30  8  3
 # 5    P1     5 19 16 15  8  3
 # 6    P1     6 13  7 11  4 10

It is useful to reorganise the above dataset in ungrouped form, so that we have one line for each starch grain (2441 lines), containing all the information about each individual starch grain (i.e. the photo, the class and the limits of the classes (sizeLow and sizeUp). This is done by using the facilities provided in the reshape package (Wickham, 2007).

moltenData <- melt(dataset, id=c("Group","Photo" ))
datasetR <- moltenData[rep(seq_len(nrow(moltenData)), moltenData$value),][,1:3]
names(datasetR)[3] <- "Class"
row.names(datasetR) <- 1:2441
rm(moltenData)

#Imputing the diameter interval for each starch grain
datasetR$sizeLow = with(datasetR, ifelse(Class=="c1", NA,
        ifelse(Class=="c2", 4,
          ifelse(Class=="c3", 8, 
            ifelse(Class=="c4", 12, 16)))) ) 
datasetR$sizeUp = with(datasetR, ifelse(Class=="c1", 4,
        ifelse(Class=="c2", 8,
          ifelse(Class=="c3", 12, 
            ifelse(Class=="c4", 16, NA)))) ) 

 #   Group Photo Class sizeLow sizeUp
 # 1    P1     1    c1      NA      4
 # 2    P1     1    c1      NA      4
 # 3    P1     1    c1      NA      4
 # 4    P1     1    c1      NA      4
 # 5    P1     1    c1      NA      4
 # 6    P1     1    c1      NA      4

The traditional method of analysis for this example would be to fit an unordered multinomial logit model. The following code

1.	loads the dataset,
2.	loads the nnet package (Venables and Ripley, 2002. The library emmeans is also necessary, but it has already been loaded)
3.	fits the multinomial model,
4.	fits a ‘null’ model, where there is no difference between producers, and
5.	compares the full and reduced models, by using a likelihood ratio test.

library(nnet)
mmod <- multinom(Class ~ Group, datasetR)
lsmeans(mmod, ~Group:Class)

 #Group Class       prob          SE df   lower.CL  upper.CL
 #P1    c1    0.26387376 0.013294604  8 0.23321635 0.2945312
 #P2    c1    0.26005662 0.011974488  8 0.23244341 0.2876698
 #P1    c2    0.29117490 0.013704010  8 0.25957340 0.3227764
 #P2    c2    0.21907767 0.011290852  8 0.19304092 0.2451144
 #P1    c3    0.24112576 0.012903510  8 0.21137022 0.2708813
 #P2    c3    0.20640863 0.011048063  8 0.18093176 0.2318855
 #P1    c4    0.12102033 0.009838293  8 0.09833319 0.1437075
 #P2    c4    0.15052247 0.009761136  8 0.12801325 0.1730317
 #P1    c5    0.08280524 0.008313059  8 0.06363529 0.1019752
 #P2    c5    0.16393459 0.010105997  8 0.14063012 0.1872391
Confidence level used: 0.95

mmodNull <- multinom(Class ~ 1, datasetR)
anova(mmod, mmodNull)
Likelihood ratio tests of Multinomial Models

#Response: Class
#  Model Resid. df Resid. Dev   Test    Df LR stat.      Pr(Chi)
#1     1      9760   7651.197                                   
#2 Group      9756   7599.133 1 vs 2     4 52.06482 1.337066e-10
 
The analyses may be more efficiently performed by fitting a survival regression method.
The following R code fits this model using a Gaussian CDF (dist=‘gaussian’). The survival package is necessary.

surv.reg1 <- survreg(Surv(sizeLow, sizeUp, type="interval2") ~ Group - 1,
 dist="gaussian", data=datasetR)
summary(surv.reg1)

# Call:
# survreg(formula = Surv(sizeLow, sizeUp, type = "interval2") ~ 
#     Group - 1, data = datasetR, dist = "gaussian")
#            Value Std. Error    z         p
# GroupP1     7.34     0.2122 34.6 3.46e-262
# GroupP2     8.67     0.1920 45.1  0.00e+00
# Log(scale)  1.89     0.0206 91.7  0.00e+00
# 
# Scale= 6.63 
# 
# Gaussian distribution
# Loglik(model)= -3824.4   Loglik(intercept only)= -3835.2
# 	Chisq= 21.57 on 1 degrees of freedom, p= 3.4e-06 
# Number of Newton-Raphson Iterations: 3 
# n= 2441

SAS code
Just wondering whether it might be useful to have SAS code for the multinomial and survival models, by using PROC LOGISTIC and PROC LIFEREG (maybe I’m wrong with the names).


SAS code here


With SAS, we can also use PROC NLMIXED. The following SAS code

1.	loads the data
2.	defines a correct likelihood, and
3.	fits the model using NLMIXED


data datasetSAS;
input  Photo Groups  n1  n2  n3  n4  n5;
n=sum(of n1-n5);
datalines;
     1      1 21 42 40 29 14
     2      1 23 45 22 12 10
     3      1 17 40 33 18  8
     4      1 24 42 30  8  3
     5      1 19 16 15  8  3
     6      1 13  7 11  4 10
     7      1 34 16  8  5  9
     8      1 23 13 10  4  1
     9      1 21 28 26  8  8
    10      1 26 27 27 14  8
    11      1 29 25 21 16  7
    12      1 40 19 22  7 10
    13      2 37 33 30 27 19
    14      2 31 40 24 20 15
    15      2 23 37 31 13 19
    16      2 32 36 30  5 20
    17      2 32 47 28 20 20
    18      2 22 18 25 22 13
    19      2 29 20 27 14 18
    20      2 28 13 21 17 11
    21      2 28 12 16 13 21
    22      2 33 13 24 12 26
    23      2 20 13 13 20 22
    24      2 34 12  8 19 16
    ;


proc nlmixed data=datasetSAS technique=nrridg;
parms sigma=4 mu1=8 mu2=10;
t1=4; t2=8; t3=12; t4=16;
if groups=1 then mu=mu1; else mu=mu2;
pi1=probnorm((t1-mu)/sigma);
pi2=probnorm((t2-mu)/sigma) - probnorm((t1-mu)/sigma);
pi3=probnorm((t3-mu)/sigma) - probnorm((t2-mu)/sigma);
pi4=probnorm((t4-mu)/sigma) - probnorm((t3-mu)/sigma);
pi5=1-probnorm((t4-mu)/sigma);
logL=lgamma(n+1)-lgamma(n1+1)-lgamma(n2+1)-lgamma(n3+1)-lgamma(n4+1)-lgamma(n5+1) + n1*log(pi1)+n2*log(pi2)+n3*log(pi3)+n4*log(pi4)+n5*log(pi5);
model n1 ~ general(logL);
estimate 'diff' mu1-mu2;
estimate ‘sigma2’ sigma*sigma;
run; 

                             The NLMIXED Procedure

                 NOTE: GCONV convergence criterion satisfied.


                                 Fit Statistics

                    -2 Log Likelihood                  671.9
                    AIC (smaller is better)            677.9
                    AICC (smaller is better)           679.1
                    BIC (smaller is better)            681.4

                               Parameter Estimates

               Standard                               95% Confidence
Par.  Estimate    Error   DF   t Value  Pr > |t|         Limits      Gradient

sigma 6.6256     0.1367   24    48.48    <.0001   6.3435    6.9076   4.6E-10
mu1   7.3398     0.2122   24    34.59    <.0001   6.9019    7.7778   5.531E-9
mu2   8.6666     0.1920   24    45.13    <.0001   8.2703    9.0629   8.598E-9



                              Additional Estimates

                    St.
Label  Estimate   Error    DF   t Value Pr > |t|    Alpha    Lower    Upper

diff   -1.3267    0.2853   24   -4.65    0.0001     0.05   -1.9156   -0.7379
sigma2 43.8985    1.8109   24   24.24    <.0001     0.05   40.1609   47.6360


Example 3: estimation of random effects
SAS code
The following SAS code:
1. defines a correct conditional likelihood, and
2. fits the random-effects model by Gaussian quadrature using NLMIXED

options linesize=100;
proc nlmixed data=datasetSAS technique=nrridg;
parms sigma=4 mu1=8 mu2=10 sigma2p=1.6;
t1=4; t2=8; t3=12; t4=16;
if groups=1 then mu=mu1; else mu=mu2;
random p ~ normal(0,sigma2p) subject=photo;
mu=mu+p;
pi1=probnorm((t1-mu)/sigma);
pi2=probnorm((t2-mu)/sigma) - probnorm((t1-mu)/sigma);
pi3=probnorm((t3-mu)/sigma) - probnorm((t2-mu)/sigma);
pi4=probnorm((t4-mu)/sigma) - probnorm((t3-mu)/sigma);
pi5=1-probnorm((t4-mu)/sigma);
logL=lgamma(n+1)-lgamma(n1+1)-lgamma(n2+1)-lgamma(n3+1)-lgamma(n4+1)-lgamma(n5+1) + n1*log(pi1)+n2*log(pi2)+n3*log(pi3)+n4*log(pi4)+n5*log(pi5);
model n1 ~ general(logL);
estimate 'diff' mu1-mu2;
estimate 'sigmap' sqrt(sigma2p);
estimate 'sigma2' sigma*sigma;
run;
                                 The NLMIXED Procedure

                       NOTE: GCONV convergence criterion satisfied.


                                     Fit Statistics

                        -2 Log Likelihood                  666.2
                        AIC (smaller is better)            674.2
                        AICC (smaller is better)           676.3
                        BIC (smaller is better)            678.9

                                   Parameter Estimates

               Stand.                      95% Confidence
Param.  Est.   Error  DF  t Value Pr>|t|      Limits         Gradient

sigma   6.5931 0.1364 23  48.33   <.0001   6.3108  6.8753    3.18E-11
mu1     7.2367 0.2918 23  24.80   <.0001   6.6330  7.8403    2.07E-10
mu2     8.6992 0.2724 23  31.93   <.0001   8.1357  9.2627    2.79E-10
sigma2p 0.4424 0.2862 23   1.55   0.1358  -0.1496  1.0345    2.53E-1

                                   Additional Estimates

                  Stand.
Label   Estimate  Error    DF   t Value Pr>|t|  Alpha  Lower     Upper

diff     -1.4625  0.3995   23   -3.66   0.0013  0.05  -2.2889   -0.6362
sigmap    0.6652  0.2151   23    3.09   0.0051  0.05   0.2201    1.1102
sigma2   43.4685  1.7990   23   24.16   <.0001  0.05  39.7470   47.1900


 
Example 3: the Bayesian perspective
R code
The first step is to specify an appropriate model (in JAGS code), which requires the following steps: 
1. a value of zero is assigned to all observations (the “zeros trick”, as explained in the text);
2. Equation (6) is used, in which the expected diameter is assigned to all N observations, by using the for() loop;
3. the likelihood function is calculated for the observations in the first class (left-censored; Equation 3), assumed to be in the first N1 rows of the database;
4. the likelihood function is calculated for the observations in the 2nd to 4th class (interval-censored; Equation 4), assumed to be in the rows from N1+1 to N2;
5. the likelihood function is calculated for the observations in the 5th class (right-censored; Equation 5), assumed to be in the rows from N2+1 to N; and
6. the prior distributions are specified for all the parameters.

# Save BUGS description of the model to working directory
modelSpec <- "
data{
for (i in 1:N) {
zeros[i] <- 0
}}

model{
for (i in 1:N) {
  #Equation 6
  exp[i] <- mu[Group[i]] + gamma[Photo[i]]
}

for (i in 1:N1) {
  #Likelihood for left-censored
  S2[i] <- pnorm(high[i], exp[i], tau.e)
  L[i] <- S2[i]     #(Equation 3)       
  phi[i] <- -(log(L[i]))
  zeros[i] ~ dpois(phi[i])    
}

for (i in (N1+1):N2) {
  #Likelihood for interval-censored
  S[i] <- pnorm(low[i], exp[i], tau.e)
  S2[i] <- pnorm(high[i], exp[i], tau.e)
  L[i] <- S2[i] - S[i]  #(Equation 4)      
  phi[i] <- -(log(L[i]))
  zeros[i] ~ dpois(phi[i])    
}

for (i in (N2+1):N) {
  #Likelihood for right-censored
  S[i] <- pnorm(low[i], exp[i], tau.e)
  L[i] <- 1 - S[i]  #(Equation 5)      
  phi[i] <- -(log(L[i]))
  zeros[i] ~ dpois(phi[i])    
}

#Priors
  sigma.e ~ dunif(0, 100)
  sigma.P ~ dunif(0, 100)
for(i in 1:2){
  mu[i] ~ dnorm(0, 0.000001)
}for(i in 1:24){
  gamma[i] ~ dnorm(0, tau.P)
}


#Derived quantities
  sigma2p <- sigma.P*sigma.P
  sigma2e <- sigma.e*sigma.e
  tau.P <- 1 / sigma2p
  tau.e <- 1 / sigma2e
  diff <- mu[1] - mu[2]
}

"


The above model is coded within R and assigned to a string of text (“modelSpec”). Using R to fit the above model requires
1. loading the rjags library (Plummer, 2016)
2. storing the model definition to an external text file (“censoredMixedModel.txt”), using the function writeLines();
3. sorting the dataset in an ascending class order so that the individuals in the first diameter class begin the data set (from row 1 to row 639) and the individuals in the 5th diameter class end it (from row 2,131 to row 2,441);
4. creating two lists: a list of all the data needed for the analysis (win.data) and a list of the initial values for the parameters to be estimated (init); and
5. sending the model specification and the other data to JAGS, using the function jags.model(), provided by the package rjags; this function returns samples from the posterior distribution for all the estimated parameters. 

library(rjags)
writeLines(modelSpec, con="censoredMixedModel.txt")
dataset_jags <- datasetR[order(datasetR$Class), ]
N1 <- 639; N2 <- 2130; N <- 2441
win.data <- list(low = dataset_jags$sizeLow, high=dataset_jags$sizeUp, 
                 N1=N1, N2=N2, N=N, Group=factor(dataset_jags$Group),
                 Photo=factor(dataset_jags$Photo))

init <- list (mu = c(7.3, 8.7), sigma.e = 1.7, sigma.P=0.5)
mcmc <- jags.model("censoredMixedModel.txt",data = win.data, inits = init, 
                   n.chains = 4, n.adapt = 100)
params <- c("mu", "sigma.e", "sigma.P", "sigma2p", "sigma2e", "diff")
res3 <- coda.samples(mcmc, params, n.iter=10000)
burn.in <- 1000
summary(window(res3, start=burn.in))

#[Output is shown in Table 7 in the main paper]

SAS code
Should we show the code for PROC MCMC?
Example 3: fitting a generalized linear model (GLM) for interval-censored data
SAS code
We can exploit the fact that the kernels of the multinomial and Poisson likelihoods are identical. Thus, we can model the observed counts  as Poisson random variables with expectation   (McCullagh and Nelder 1989; §6.4.2; Keen and Engel, 1997).
The cutpoints   are known. If the variance   were known as well, we would have had a regular GLM with link functions given by the Equation (4) to (6) in the main paper. We can use the ML estimate of the variance; below, we try this in GLIMMIX—and it works perfectly.


/*can we fit threshold model using the known thresholds?*/
data StarchGrainsSAS_Counts;
set datasetSAS;
count=n1; cat=1; output;
count=n2; cat=2; output;
count=n3; cat=3; output;
count=n4; cat=4; output;
count=n5; cat=5; output;
run;

/*yes we can if the variance is known!*/
proc glimmix data=StarchGrainsSAS_Counts method=quad;
class groups photo;
sigma=6.5931;
t1=4; t2=8; t3=12; t4=16;
if cat=1 then _mu_=n*probnorm((t1-_linp_)/sigma);
if cat=2 then _mu_=n*( probnorm((t2-_linp_)/sigma) - probnorm((t1-_linp_)/sigma) );
if cat=3 then _mu_=n*( probnorm((t3-_linp_)/sigma) - probnorm((t2-_linp_)/sigma) );
if cat=4 then _mu_=n*( probnorm((t4-_linp_)/sigma) - probnorm((t3-_linp_)/sigma) );
if cat=5 then _mu_=n*(1-probnorm((t4-_linp_)/sigma));
model count=groups/dist=poisson solution;
random int/sub=photo;
run;
                                          Fit Statistics

                                -2 Log Likelihood             820.31
                                AIC  (smaller is better)      826.31
                                AICC (smaller is better)      826.51
                                BIC  (smaller is better)      829.84
                                CAIC (smaller is better)      832.84
                                HQIC (smaller is better)      827.24


                                   Fit Statistics for Conditional
                                            Distribution

                              -2 log L(count | r. effects)      793.55
                              Pearson Chi-Square                223.08
                              Pearson Chi-Square / DF             1.86


                                   Covariance Parameter Estimates

                                                                Standard
                            Cov Parm     Subject    Estimate       Error

                            Intercept    Photo        0.4424      0.2862


                                    Solutions for Fixed Effects

                                               Standard
Effect       Groups    Estimate       Error       DF    t Value    Pr > |t|

Intercept                8.6992      0.2721       22      31.97      <.0001
Groups       1          -1.4625      0.3994       96      -3.66      0.0004
Groups       2                0           .        .        .         .

This agrees almost exactly with the corresponding output from NLMIXED (apart from the "residual" degrees of freedom). 
The only problem is how to estimate the variance. A simple option is a grid search for the optimal value of   . Another idea is to use Pregibon's (1980) approach for models with parameters in the link function (also see McCullagh and Nelder, 1989; § 11.3), but this is tricky because the link function for the Poisson approach looks messy. 
Here is the solution within a GLMM framework. If we blow up the data to the level of individual grains, we can fit a random grain effect   in order to properly model and estimate the residual variance. At an individual grain level (that is, for a particular grain), we have   and the observed count is zero for all categories except for the category the grain falls into; for this category, the count equals one. 
If we use a probit link with unit variance, the total residual variance will be parameterized as  . This works whenever σ² ≥ 1 (in this example  ). If  , we need to use a scale parameter   in the probit link. Our model can then be stated as follows:

 ,	(7)
 	(8)
 	(9)
 	(10)
 	(11)

Then, conditionally on the random effect  , the multinomial probabilities are
 ,	(12)
    for   , and	(13)
 .	(14)

To implement the above idea, we need to supply the user-defined link functions (12) to (14) and play with the scaling factor so that  . In our example, this holds for  .


/*Another idea: Blow up the data to individual grains and fit additional random effect for units*/

data StarchGrainsSAS_Individual;
set StarchGrainsSAS_Counts;
n=1;
do grain=1 to count;
  do cat2=1 to 5;
    if cat2=cat then count2=1; else count2=0; 
    output;
  end;
end;
run;

proc glimmix data=StarchGrainsSAS_Individual method=quad;
class groups photo grain cat;
theta=4;
t1=4; t2=8; t3=12; t4=16;
if cat2=1 then _mu_=n*probnorm((t1-_linp_)/theta);
if cat2=2 then _mu_=n*( probnorm((t2-_linp_)/theta) - probnorm((t1-_linp_)/theta) );
if cat2=3 then _mu_=n*( probnorm((t3-_linp_)/theta) - probnorm((t2-_linp_)/theta) );
if cat2=4 then _mu_=n*( probnorm((t4-_linp_)/theta) - probnorm((t3-_linp_)/theta) );
if cat2=5 then _mu_=n*(1-probnorm((t4-_linp_)/theta));
model count2=groups/dist=poisson solution;
random int/sub=photo*grain*cat;
run;

                        Convergence criterion (GCONV=1E-8) satisfied.


                                        Fit Statistics

                             -2 Log Likelihood           12531.05
                             AIC  (smaller is better)    12537.05
                             AICC (smaller is better)    12537.05
                             BIC  (smaller is better)    12554.45
                             CAIC (smaller is better)    12557.45
                             HQIC (smaller is better)    12543.37


                                Fit Statistics for Conditional
                                         Distribution

                           -2 log L(count2 | r. effects)     9173.19
                           Pearson Chi-Square                3549.85
                           Pearson Chi-Square / DF              0.29


                                Covariance Parameter Estimates

                                                                 Standard
                     Cov Parm     Subject            Estimate       Error

                     Intercept    Photo*grain*cat     27.8470      1.8031


                                  Solutions for Fixed Effects

                                             Standard
Effect       Groups    Estimate       Error       DF    t Value    Pr > |t|

Intercept                8.6670      0.1919     2439      45.16      <.0001
Groups       1          -1.3264      0.2851     9764      -4.65      <.0001


                                     The GLIMMIX Procedure

                                  Solutions for Fixed Effects

                                             Standard
 Effect       Groups    Estimate       Error       DF    t Value    Pr > |t|
 Groups       2                0           .        .        .         .


                                Type III Tests of Fixed Effects

                                      Num      Den
                        Effect         DF       DF    F Value    Pr > F
                        Groups          1     9764      21.64    <.0001


The above result agrees with the full ML estimates obtained with NLMIXED. Adding a random photo effect, we obtain the following result:


/*add random photo effect*/
proc glimmix data=StarchGrainsSAS_Individual method=laplace;
class groups photo grain cat;
theta=4;
t1=4; t2=8; t3=12; t4=16;
if cat2=1 then _mu_=n*probnorm((t1-_linp_)/theta);
if cat2=2 then _mu_=n*( probnorm((t2-_linp_)/theta) - probnorm((t1-_linp_)/theta) );
if cat2=3 then _mu_=n*( probnorm((t3-_linp_)/theta) - probnorm((t2-_linp_)/theta) );
if cat2=4 then _mu_=n*( probnorm((t4-_linp_)/theta) - probnorm((t3-_linp_)/theta) );
if cat2=5 then _mu_=n*(1-probnorm((t4-_linp_)/theta));
model count2=groups/dist=poisson solution;
random int/sub=photo;
random grain*cat/sub=photo;
run;

                        Convergence criterion (GCONV=1E-8) satisfied.


                                        Fit Statistics

                             -2 Log Likelihood           12550.22
                             AIC  (smaller is better)    12558.22
                             AICC (smaller is better)    12558.22
                             BIC  (smaller is better)    12562.93
                             CAIC (smaller is better)    12566.93
                             HQIC (smaller is better)    12559.47


                                Fit Statistics for Conditional
                                         Distribution

                           -2 log L(count2 | r. effects)     9269.27
                           Pearson Chi-Square                3659.44
                           Pearson Chi-Square / DF              0.30


                                Covariance Parameter Estimates

                                                             Standard
                         Cov Parm     Subject    Estimate       Error

                         Intercept    Photo        0.4184      0.2710
                         grain*cat    Photo       25.3592      1.6184


                                     The GLIMMIX Procedure

                                  Solutions for Fixed Effects

                                             Standard
Effect       Groups    Estimate       Error       DF    t Value    Pr > |t|
Intercept                8.7166      0.2653       22      32.85      <.0001
Groups       1          -1.4475      0.3892     9764      -3.72      0.0002
Groups       2                0           .        .        .         .


                                Type III Tests of Fixed Effects

                                      Num      Den
                        Effect         DF       DF    F Value    Pr > F
                        Groups          1     9764      13.83    0.0002


To get this to run, we had to use the Laplace method instead of adaptive Gaussian quadrature with three quadrature points; the computer, unfortunately, went out of memory. It is likely due to the smaller accuracy of the Laplace approximation this approach uses that the results do not agree so well with those of NLMIXED.
So, this approach works and has all the virtues of using a GLMM framework, but it is computationally more demanding than the direct approach using NLMIXED.

References
H. Wickham. Reshaping data with the reshape package. Journal of Statistical Software, 21(12), 2007.
Russell Lenth (2018). emmeans: Estimated Marginal Means, aka Least-Squares Means. R package version 1.2.3. https://CRAN.R-project.org/package=emmeans
Terry M. Therneau, Patricia M. Grambsch (2000). _Modeling
Survival Data: Extending the Cox Model_. Springer, New York.
ISBN 0-387-98784-3.

Venables, W. N. & Ripley, B. D. (2002) Modern Applied
  Statistics with S. Fourth Edition. Springer, New York.
  ISBN 0-387-95457-0

Martyn Plummer (2016). rjags: Bayesian Graphical Models
  using MCMC. R package version 4-6.
  https://CRAN.R-project.org/package=rjags

