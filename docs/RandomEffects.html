<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="A. Onofri, H-P. Piepho and Marcin Kozak" />


<title>Example 3. Fitting a mixed effect survival model</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = false;
    options.smoothScroll = false;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}

.tocify-subheader {
  display: inline;
}
.tocify-subheader .tocify-item {
  font-size: 0.95em;
}

</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">agriCensData</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="Example1.html">Example 1</a>
</li>
<li>
  <a href="Example2.html">Example 2</a>
</li>
<li>
  <a href="Example3.html">Example 3</a>
</li>
<li>
  <a href="RandomEffects.html">Random effects</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Example 3. Fitting a mixed effect survival model</h1>
<h4 class="author"><em>A. Onofri, H-P. Piepho and Marcin Kozak</em></h4>

</div>


<hr />
<div id="the-model-definition-in-jags-code" class="section level1">
<h1>The model definition (in JAGS code)</h1>
<p>In the previous session ( <a href="https://onofriandreapg.github.io/agriCensData/Example3.html">here</a> we have fitted a survival model to a dataset relating to potato starch grains assessed in size categories. Our analyses assumed that starch grains were independent, but they are not, as they are grouped by photo. Indeed, starch grains observed in the same photo can be correlated and thus similar to each other. In our paper, we discuss that neglecting this correlation we erroneously treated the 2,441 starch grains as 2,441 independent pieces of information, leading to overly small standard errors of our estimates.</p>
<p>Including random effects in a survival model may be rather difficult, unless we use Bayesian methods. In our main paper, we already discussed the differences between traditional and Bayesian statistics; therefore, we will not make this point again here and we will concentrate on the fitting process. The model that we are going to fit is as follows:</p>
<p><span class="math display">\[ y_{ijk} = \mu_j+ \gamma_k + \varepsilon_{ijk} \]</span></p>
<p>where <span class="math inline">\(y_{ijk}\)</span> i the diameter for the <span class="math inline">\(i-th\)</span> grain in the <span class="math inline">\(k-th\)</span> phot for the <span class="math inline">\(j-th\)</span> producer, <span class="math inline">\(\mu_j\)</span> is the mean diameter for the j-th producer, <span class="math inline">\(\gamma_k\)</span> is the random error for the <span class="math inline">\(k-th\)</span> photo, and <span class="math inline">\(\varepsilon_{ijk}\)</span> is the residual error term. We will assume that <span class="math inline">\(\varepsilon\)</span> is homoscedastic and normally distributed, with mean equal to zero and standard deviation equal to <span class="math inline">\(\sigma\)</span>. We will also assume that <span class="math inline">\(\gamma\)</span> (random photo effect) is normally distributed, independent on <span class="math inline">\(\epsilon\)</span>, with mean equal to 0 and standard deviation equal to <span class="math inline">\(\sigma_p\)</span>. This latter represents the photo-to-photo variability while the residual standard deviation represents the within-group (within-photo) variability.</p>
<p>In order to fit the above model, we used the MCMC sampler provided with the freeware software JAGS (Just Another Gibbs Sampler), which can be invoked from an R session, by using the package ‘rjags’ <span class="citation">(Plummer 2016)</span>.</p>
<p>The first step to the analysis is to specify a model (in JAGS code) in a text string (‘modelSpec’). This definition is reported below.</p>
<pre class="r"><code># Save BUGS description of the model as a string
modelSpec &lt;- &quot;
data{
for (i in 1:N) {
zeros[i] &lt;- 0
}}

model{
for (i in 1:N) {
  exp[i] &lt;- mu[Group[i]] + gamma[Photo[i]]
}

for (i in 1:N1) {
  #Likelihood for left-censored
  S2[i] &lt;- pnorm(high[i], exp[i], tau.e)
  L[i] &lt;- S2[i]     #(Equation 3)       
  phi[i] &lt;- -(log(L[i]))
  zeros[i] ~ dpois(phi[i])    
}

for (i in (N1+1):N2) {
  #Likelihood for interval-censored
  S[i] &lt;- pnorm(low[i], exp[i], tau.e)
  S2[i] &lt;- pnorm(high[i], exp[i], tau.e)
  L[i] &lt;- S2[i] - S[i]  #(Equation 4)      
  phi[i] &lt;- -(log(L[i]))
  zeros[i] ~ dpois(phi[i])    
}

for (i in (N2+1):N) {
  #Likelihood for right-censored
  S[i] &lt;- pnorm(low[i], exp[i], tau.e)
  L[i] &lt;- 1 - S[i]  #(Equation 5)      
  phi[i] &lt;- -(log(L[i]))
  zeros[i] ~ dpois(phi[i])    
}

#Priors
  sigma.e ~ dunif(0, 100)
  sigma.P ~ dunif(0, 100)
for(i in 1:2){
  mu[i] ~ dnorm(0, 0.000001)
}for(i in 1:24){
  gamma[i] ~ dnorm(0, tau.P)
}

#Derived quantities
  sigma2p &lt;- sigma.P*sigma.P
  sigma2e &lt;- sigma.e*sigma.e
  tau.P &lt;- 1 / sigma2p
  tau.e &lt;- 1 / sigma2e
  diff &lt;- mu[1] - mu[2]
}
&quot;</code></pre>
<p>This definition contains an initial data step, where we create a new variable (‘zeros[i]’) by assigning a value of 0 to all observations. The reason why this data step is necessary will be clear later on.</p>
<p>Successively, we have a model step, where we code the model that we want to fit. This returns the expected value for each observation (‘exp[i]’). Note the coding with double square brackets (‘mu[Group[i]]’) that is used to mean that ‘mu’ takes different values, depending on ‘Group[i]’.</p>
<p>The next part codes the log-likelihood for the i-th observation. Please note that the observations are assumed to be sorted out, so that right-censored observations take the positions from 1 to N1, interval censored observations go from N1 to N2 and left-censored observations go from N2 to the end of the dataset. For each group, the negative log-likelihood is coded according to the Equations from 3 to 5 in our main paper and it is stored in the variable ‘phi’.</p>
<p>At this stage, we need to make JAGS use ‘phi’ to calculate the likelihood for each observation. Such calculation would seem easy, as the likelihood is equal to ‘exp(-phi)’. Unfortunately, JAGS/BUGS can only calculate the likelihood by using a few pre-defined functions that do not provide a simple direct solution.</p>
<p>This problem can be solved by using the so-called ‘zeros trick’ <span class="citation">(Spiegelhalter et al. 2003)</span>: the trick is to set the observed value for the i-th observation as zeros[i] = 0, which we have done with the first data step. We now need to specify that the observed value (0 for all observations) is distributed according to a Poisson probability function (i.e. one of the available pre-defined functions) with mean equal to phi; we can do this with the command zeros[i] ~ dpois(phi[i]). Note that the probability of obtaining zero from a Poisson distribution with mean equal to phi is exactly exp(-phi). Therefore, with this ‘zeros trick’ the software can assign the correct likelihood to each individual observation.</p>
<p>Finally, the priors for all estimands are defined. For both variance parameters, we selected a uniform distribution from 0 to 100 [sigma.e ~ dunif(0, 100)]. For <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\mu_2\)</span> our prior expectation was that they were normally distributed with a mean of 0 and a very high standard deviation (<span class="math inline">\(\sigma\)</span> = 103). So high a value of the standard deviation used a prior information means that, without looking at the experimental data, we had no idea about the values assumed by the unknown parameters. The priors for <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\mu_2\)</span> were set with the command mu[i] ~ dnorm(0, 0.000001), which specifies the normal distribution using the mean (0) and the precision (0.000001), which is equal to <span class="math inline">\(1/\sigma^2\)</span>. Such a parameterisation is used by JAGS and other MCMC samplers, such as WinBUGS.</p>
<p>In the above model definition we also added some derived quantities, to be calculated from the estimated parameters, such as the difference between <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\mu_2\)</span>.</p>
<hr />
</div>
<div id="model-fitting-in-r" class="section level1">
<h1>Model fitting in R</h1>
<p>Once the models has been coded and assigned to a string of text (“modelSpec”), before fitting the model, we have to</p>
<ol style="list-style-type: decimal">
<li>store the model definition to an external text file (“censoredMixedModel.txt”), using the function writeLines()</li>
<li>load the dataset. We will reuse the dataset in the ungrouped form, as used for the survival model fit. This is available in the package agriCensData as starchGrainU</li>
<li>sort the dataset in an ascending class order so that the individuals in the first diameter class (left-censored) begin the data set (from row 1 to row 639) and the individuals in the 5th diameter class (right-censored) end it (from row 2,131 to row 2,441)</li>
<li>Define a few variables that are used in the model definition, such as N (the number of observations), N1 (the number of left-censored observations, i.e. 639) and N2 (the number of left-censored + interval censored observations, i.e. 2130).</li>
</ol>
<pre class="r"><code>library(agriCensData)
writeLines(modelSpec, con=&quot;censoredMixedModel.txt&quot;)
data(starchGrainU)
dataset_jags &lt;- starchGrainU[order(starchGrainU$Class), ]
N1 &lt;- 639; N2 &lt;- 2130; N &lt;- 2441</code></pre>
<p>Now we are ready to fit the model. To this aim we</p>
<ol style="list-style-type: decimal">
<li>load the rjags library</li>
<li>create two lists: a list of all the data needed for the analysis (win.data) and a list of the initial values for the parameters to be estimated (init); and</li>
<li>sending the model specification and the other data to JAGS, using the function jags.model(), provided by the package rjags; this function, in a few seconds, returns samples from the posterior distribution for all the estimated parameters (‘res3’).</li>
</ol>
<p>Once the samples from the posterior have been obtained, we specify that 1000 samples should be discarded as ‘burn.in’. This samples might have been produced before reaching the convergence and, therefore, they might not come from the correct posterior distribution.</p>
<p>From the posterior, we obtain the mean and median as measures of central tendency, the standard deviation as a measure of uncertainty, and credible intervals, which are the Bayesian analog to confidence intervals.</p>
<pre class="r"><code>library(rjags)
win.data &lt;- list(low = dataset_jags$sizeLow, high=dataset_jags$sizeUp, 
                 N1=N1, N2=N2, N=N, Group=factor(dataset_jags$Group),
                 Photo=factor(dataset_jags$Photo))

init &lt;- list (mu = c(7.3, 8.7), sigma.e = 1.7, sigma.P=0.5)
mcmc &lt;- jags.model(&quot;censoredMixedModel.txt&quot;,data = win.data, inits = init, 
                   n.chains = 4, n.adapt = 100)</code></pre>
<pre><code>## Compiling data graph
##    Resolving undeclared variables
##    Allocating nodes
##    Initializing
##    Reading data back into data table
## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 2441
##    Unobserved stochastic nodes: 28
##    Total graph size: 13388
## 
## Initializing model
## 
## 
  |                                                        
  |                                                  |   0%
  |                                                        
  |+                                                 |   2%
  |                                                        
  |++                                                |   4%
  |                                                        
  |+++                                               |   6%
  |                                                        
  |++++                                              |   8%
  |                                                        
  |+++++                                             |  10%
  |                                                        
  |++++++                                            |  12%
  |                                                        
  |+++++++                                           |  14%
  |                                                        
  |++++++++                                          |  16%
  |                                                        
  |+++++++++                                         |  18%
  |                                                        
  |++++++++++                                        |  20%
  |                                                        
  |+++++++++++                                       |  22%
  |                                                        
  |++++++++++++                                      |  24%
  |                                                        
  |+++++++++++++                                     |  26%
  |                                                        
  |++++++++++++++                                    |  28%
  |                                                        
  |+++++++++++++++                                   |  30%
  |                                                        
  |++++++++++++++++                                  |  32%
  |                                                        
  |+++++++++++++++++                                 |  34%
  |                                                        
  |++++++++++++++++++                                |  36%
  |                                                        
  |+++++++++++++++++++                               |  38%
  |                                                        
  |++++++++++++++++++++                              |  40%
  |                                                        
  |+++++++++++++++++++++                             |  42%
  |                                                        
  |++++++++++++++++++++++                            |  44%
  |                                                        
  |+++++++++++++++++++++++                           |  46%
  |                                                        
  |++++++++++++++++++++++++                          |  48%
  |                                                        
  |+++++++++++++++++++++++++                         |  50%
  |                                                        
  |++++++++++++++++++++++++++                        |  52%
  |                                                        
  |+++++++++++++++++++++++++++                       |  54%
  |                                                        
  |++++++++++++++++++++++++++++                      |  56%
  |                                                        
  |+++++++++++++++++++++++++++++                     |  58%
  |                                                        
  |++++++++++++++++++++++++++++++                    |  60%
  |                                                        
  |+++++++++++++++++++++++++++++++                   |  62%
  |                                                        
  |++++++++++++++++++++++++++++++++                  |  64%
  |                                                        
  |+++++++++++++++++++++++++++++++++                 |  66%
  |                                                        
  |++++++++++++++++++++++++++++++++++                |  68%
  |                                                        
  |+++++++++++++++++++++++++++++++++++               |  70%
  |                                                        
  |++++++++++++++++++++++++++++++++++++              |  72%
  |                                                        
  |+++++++++++++++++++++++++++++++++++++             |  74%
  |                                                        
  |++++++++++++++++++++++++++++++++++++++            |  76%
  |                                                        
  |+++++++++++++++++++++++++++++++++++++++           |  78%
  |                                                        
  |++++++++++++++++++++++++++++++++++++++++          |  80%
  |                                                        
  |+++++++++++++++++++++++++++++++++++++++++         |  82%
  |                                                        
  |++++++++++++++++++++++++++++++++++++++++++        |  84%
  |                                                        
  |+++++++++++++++++++++++++++++++++++++++++++       |  86%
  |                                                        
  |++++++++++++++++++++++++++++++++++++++++++++      |  88%
  |                                                        
  |+++++++++++++++++++++++++++++++++++++++++++++     |  90%
  |                                                        
  |++++++++++++++++++++++++++++++++++++++++++++++    |  92%
  |                                                        
  |+++++++++++++++++++++++++++++++++++++++++++++++   |  94%
  |                                                        
  |++++++++++++++++++++++++++++++++++++++++++++++++  |  96%
  |                                                        
  |+++++++++++++++++++++++++++++++++++++++++++++++++ |  98%
  |                                                        
  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%</code></pre>
<pre class="r"><code>params &lt;- c(&quot;mu&quot;, &quot;sigma.e&quot;, &quot;sigma.P&quot;, &quot;sigma2p&quot;, &quot;sigma2e&quot;, &quot;diff&quot;)
res3 &lt;- coda.samples(mcmc, params, n.iter=10000)</code></pre>
<pre><code>## 
  |                                                        
  |                                                  |   0%
  |                                                        
  |*                                                 |   2%
  |                                                        
  |**                                                |   4%
  |                                                        
  |***                                               |   6%
  |                                                        
  |****                                              |   8%
  |                                                        
  |*****                                             |  10%
  |                                                        
  |******                                            |  12%
  |                                                        
  |*******                                           |  14%
  |                                                        
  |********                                          |  16%
  |                                                        
  |*********                                         |  18%
  |                                                        
  |**********                                        |  20%
  |                                                        
  |***********                                       |  22%
  |                                                        
  |************                                      |  24%
  |                                                        
  |*************                                     |  26%
  |                                                        
  |**************                                    |  28%
  |                                                        
  |***************                                   |  30%
  |                                                        
  |****************                                  |  32%
  |                                                        
  |*****************                                 |  34%
  |                                                        
  |******************                                |  36%
  |                                                        
  |*******************                               |  38%
  |                                                        
  |********************                              |  40%
  |                                                        
  |*********************                             |  42%
  |                                                        
  |**********************                            |  44%
  |                                                        
  |***********************                           |  46%
  |                                                        
  |************************                          |  48%
  |                                                        
  |*************************                         |  50%
  |                                                        
  |**************************                        |  52%
  |                                                        
  |***************************                       |  54%
  |                                                        
  |****************************                      |  56%
  |                                                        
  |*****************************                     |  58%
  |                                                        
  |******************************                    |  60%
  |                                                        
  |*******************************                   |  62%
  |                                                        
  |********************************                  |  64%
  |                                                        
  |*********************************                 |  66%
  |                                                        
  |**********************************                |  68%
  |                                                        
  |***********************************               |  70%
  |                                                        
  |************************************              |  72%
  |                                                        
  |*************************************             |  74%
  |                                                        
  |**************************************            |  76%
  |                                                        
  |***************************************           |  78%
  |                                                        
  |****************************************          |  80%
  |                                                        
  |*****************************************         |  82%
  |                                                        
  |******************************************        |  84%
  |                                                        
  |*******************************************       |  86%
  |                                                        
  |********************************************      |  88%
  |                                                        
  |*********************************************     |  90%
  |                                                        
  |**********************************************    |  92%
  |                                                        
  |***********************************************   |  94%
  |                                                        
  |************************************************  |  96%
  |                                                        
  |************************************************* |  98%
  |                                                        
  |**************************************************| 100%</code></pre>
<pre class="r"><code>burn.in &lt;- 1000
summary(window(res3, start=burn.in))</code></pre>
<pre><code>## 
## Iterations = 1000:10100
## Thinning interval = 1 
## Number of chains = 4 
## Sample size per chain = 9101 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##            Mean     SD  Naive SE Time-series SE
## diff    -1.4797 0.4385 0.0022984      0.0060028
## mu[1]    7.2226 0.3219 0.0016871      0.0042741
## mu[2]    8.7023 0.3059 0.0016035      0.0043967
## sigma.P  0.7694 0.2640 0.0013836      0.0068681
## sigma.e  6.6034 0.1386 0.0007267      0.0009359
## sigma2e 43.6236 1.8333 0.0096086      0.0124171
## sigma2p  0.6616 0.4395 0.0023032      0.0087715
## 
## 2. Quantiles for each variable:
## 
##             2.5%     25%     50%     75%   97.5%
## diff    -2.36711 -1.7622 -1.4744 -1.1940 -0.6225
## mu[1]    6.57341  7.0151  7.2270  7.4352  7.8480
## mu[2]    8.10517  8.5074  8.7004  8.8966  9.3210
## sigma.P  0.25858  0.5980  0.7602  0.9322  1.3189
## sigma.e  6.33838  6.5089  6.6012  6.6950  6.8809
## sigma2e 40.17503 42.3652 43.5762 44.8224 47.3471
## sigma2p  0.06686  0.3576  0.5778  0.8691  1.7394</code></pre>
<p>We can see that the the expected difference between the producers is equal to -1.477 and there is a 95% posterior probability that the true difference is between -2.3763 and -0.6164 and therefore it is different from 0 (please note that other runs of the model may bring to slightly different results). It is also interesting to note that the random effect for the photos has a variance component of 0.66, and its credible interval does not contain the 0.</p>
<hr />
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-Plummer:2016aa">
<p><span style="font-variant: small-caps;">Plummer, M</span> (2016) <em>Rjags: Bayesian graphical models using mcmc</em></p>
</div>
<div id="ref-spiegelhalter2003_WinBUGSusermanual">
<p><span style="font-variant: small-caps;">Spiegelhalter, D, A Thomas, N Best, D Lunn</span> (2003) <em>WinBUGS user manual</em>. version</p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
