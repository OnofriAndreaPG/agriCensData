<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="A. Onofri, H-P. Piepho and Marcin Kozak" />


<title>Example 1. Analyzing visually recorded cover-abundance data</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">agriCensData</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="Example1.html">Example 1</a>
</li>
<li>
  <a href="Example2.html">Example 2</a>
</li>
<li>
  <a href="Example3.html">Example 3</a>
</li>
<li>
  <a href="RandomEffects.html">Random effects</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Example 1. Analyzing visually recorded cover-abundance data</h1>
<h4 class="author"><em>A. Onofri, H-P. Piepho and Marcin Kozak</em></h4>

</div>


<div id="the-dataset" class="section level1">
<h1>The dataset</h1>
<p>The aim of this document is to present the code used to analyse the three examples on the main paper. The interested readers will find an extended version of this document, in the form of an Rmarkdown tutorial, at ………<br />
Example 1: Visually recorded cover-abundance data The dataset refers to a field experiment to compare weed control ability of nine post-emergence herbicides against Sorghum halepense in maize. Three weeks after the treatment, the cover-abundance of S. halepense was visually recorded in six classes, following the Braun-Blanquet method. The limits of the classes are shown as ‘L’ (lower limit) and ‘U’ (upper limit). The ‘midPoint’ represent the center of each class.</p>
<p>R code The first step to accomplish the analyses is to read the dataset into R:</p>
<p>BBsurvey &lt;- data.frame( Herbicide = factor(rep(LETTERS[1:9], each=4)), L = c(0.1, 0.1, 5, 5, 0.1, 0.1, 5, 5, 0, 0.1, 5, 5, 0, 0.1, 5, 5, 0, 0, 0.1, 0.1, 5, 5, 25, 25, 0.1, 0.1, 5, 5, 0, 0.1, 5, 5, 75, 25, 50, 25), U = c(5, 5, 25, 25, 5, 5, 25, 25, 0.1, 5, 25, 25, 0.1, 5, 25, 25, 0.1, 0.1, 5, 5, 25, 25, 50, 50, 5, 5, 25, 25, 0.1, 5, 25, 25, 100, 50, 75, 50), midPoint = c(2.55, 2.55, 15, 15, 2.55, 2.55, 15, 15, 0.05, 2.5, 15, 15, 0.05, 2.55, 15, 15, 0.05, 0.05, 2.55, 2.55, 15, 15, 37.5, 37.5, 2.5, 2.55, 15, 15, 0.1, 2.55, 15, 15, 87.5, 37.5, 62.5, 37.5) )</p>
<p>head(BBsurvey) # Herbicide L U midPoint # 1 A 0.1 5 2.55 # 2 A 0.1 5 2.55 # 3 A 5.0 25 15.00 # 4 A 5.0 25 15.00 # 5 B 0.1 5 2.55 # 6 B 0.1 5 2.55</p>
<p>Once the dataset has been read, a traditional ANOVA model can be fit, by using the mid point for each class as the dependent variable. Means and multiple comparison testing may be obtained by using the emmeans package (Lenth, 2018)</p>
<p>library(emmeans) mod.aov &lt;- lm(midPoint ~ Herbicide, data = BBsurvey) means &lt;- emmeans(mod.aov, ~Herbicide) cld(means, Letter = LETTERS, sort=F)</p>
</div>
<div id="herbicide-emmean-se-df-lower.cl-upper.cl-.group" class="section level1">
<h1>Herbicide emmean SE df lower.CL upper.CL .group</h1>
</div>
<div id="a-8.7750-5.501751-27--2.513661-20.06366-a" class="section level1">
<h1>A 8.7750 5.501751 27 -2.513661 20.06366 A</h1>
</div>
<div id="b-8.7750-5.501751-27--2.513661-20.06366-a" class="section level1">
<h1>B 8.7750 5.501751 27 -2.513661 20.06366 A</h1>
</div>
<div id="c-8.1375-5.501751-27--3.151161-19.42616-a" class="section level1">
<h1>C 8.1375 5.501751 27 -3.151161 19.42616 A</h1>
</div>
<div id="d-8.1500-5.501751-27--3.138661-19.43866-a" class="section level1">
<h1>D 8.1500 5.501751 27 -3.138661 19.43866 A</h1>
</div>
<div id="e-1.3000-5.501751-27--9.988661-12.58866-a" class="section level1">
<h1>E 1.3000 5.501751 27 -9.988661 12.58866 A</h1>
</div>
<div id="f-26.2500-5.501751-27-14.961339-37.53866-a" class="section level1">
<h1>F 26.2500 5.501751 27 14.961339 37.53866 A</h1>
</div>
<div id="g-8.7625-5.501751-27--2.526161-20.05116-a" class="section level1">
<h1>G 8.7625 5.501751 27 -2.526161 20.05116 A</h1>
</div>
<div id="h-8.1625-5.501751-27--3.126161-19.45116-a" class="section level1">
<h1>H 8.1625 5.501751 27 -3.126161 19.45116 A</h1>
</div>
<div id="i-56.2500-5.501751-27-44.961339-67.53866-b" class="section level1">
<h1>I 56.2500 5.501751 27 44.961339 67.53866 B</h1>
</div>
<div id="section" class="section level1">
<h1></h1>
</div>
<div id="confidence-level-used-0.95" class="section level1">
<h1>Confidence level used: 0.95</h1>
</div>
<div id="p-value-adjustment-tukey-method-for-comparing-a-family-of-9-estimates" class="section level1">
<h1>P value adjustment: tukey method for comparing a family of 9 estimates</h1>
</div>
<div id="significance-level-used-alpha-0.05" class="section level1">
<h1>significance level used: alpha = 0.05</h1>
<p>To fit a survival model, we use the limits of each class as the dependent variables, instead of ‘midPoint’. In order to fit this model, we need to load the survival package (Therneau, 2015).</p>
<p>library(survival) mod.surv &lt;- survreg(Surv(L, U, type=“interval2”) ~ Herbicide, dist=“gaussian”, data = BBsurvey) means.surv &lt;- emmeans(mod.surv, ~Herbicide) cld(means.surv, Letters = LETTERS, sort=F)</p>
</div>
<div id="herbicide-emmean-se-df-lower.cl-upper.cl-.group-1" class="section level1">
<h1>Herbicide emmean SE df lower.CL upper.CL .group</h1>
</div>
<div id="a-6.755275-3.684287-26--0.8178844-14.328434-ab" class="section level1">
<h1>A 6.755275 3.684287 26 -0.8178844 14.328434 AB</h1>
</div>
<div id="b-6.755275-3.684287-26--0.8178844-14.328434-ab" class="section level1">
<h1>B 6.755275 3.684287 26 -0.8178844 14.328434 AB</h1>
</div>
<div id="c-5.903803-3.631445-26--1.5607388-13.368345-a" class="section level1">
<h1>C 5.903803 3.631445 26 -1.5607388 13.368345 A</h1>
</div>
<div id="d-5.903803-3.631445-26--1.5607388-13.368345-a" class="section level1">
<h1>D 5.903803 3.631445 26 -1.5607388 13.368345 A</h1>
</div>
<div id="e-1.269626-3.253208-26--5.4174388-7.956691-a" class="section level1">
<h1>E 1.269626 3.253208 26 -5.4174388 7.956691 A</h1>
</div>
<div id="f-25.022493-4.008432-26-16.7830434-33.261942-b" class="section level1">
<h1>F 25.022493 4.008432 26 16.7830434 33.261942 B</h1>
</div>
<div id="g-6.755275-3.684287-26--0.8178844-14.328434-ab" class="section level1">
<h1>G 6.755275 3.684287 26 -0.8178844 14.328434 AB</h1>
</div>
<div id="h-5.903803-3.631445-26--1.5607388-13.368345-a" class="section level1">
<h1>H 5.903803 3.631445 26 -1.5607388 13.368345 A</h1>
</div>
<div id="i-57.335193-3.772500-26-49.5807077-65.089678-c" class="section level1">
<h1>I 57.335193 3.772500 26 49.5807077 65.089678 C</h1>
</div>
<div id="section-1" class="section level1">
<h1></h1>
</div>
<div id="results-are-given-on-the-surv-not-the-response-scale." class="section level1">
<h1>Results are given on the Surv (not the response) scale.</h1>
</div>
<div id="confidence-level-used-0.95-1" class="section level1">
<h1>Confidence level used: 0.95</h1>
</div>
<div id="p-value-adjustment-tukey-method-for-comparing-a-family-of-9-estimates-1" class="section level1">
<h1>P value adjustment: tukey method for comparing a family of 9 estimates</h1>
</div>
<div id="significance-level-used-alpha-0.05-1" class="section level1">
<h1>significance level used: alpha = 0.05</h1>
<p>SAS Code Here, it would be nice to give the code to perform with SAS the very same analyses as before (read the data, make an ANOVA and fit a survival model). Or we simply skip this part Example 2: Time-to-event data This dataset refers to a germination assay at two temperature levels (15 and 25°C). Germinated seeds were counted and removed on 20 dates after the beginning of the assay.</p>
<p>R code Also for this example, the first step consists of reading the dataset into R:</p>
<p>germination &lt;- data.frame( obsT = c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39), cumProp = c(0.02, 0.14, 0.25, 0.38, 0.52, 0.56, 0.62, 0.66, 0.71, 0.76, 0.78, 0.79, 0.81, 0.82, 0.85, 0.87, 0.89, 0.91, 0.91, 0.92, 0.1, 0.36, 0.49, 0.58, 0.68, 0.71, 0.77, 0.81, 0.82, 0.84, 0.86, 0.89, 0.89, 0.9, 0.92, 0.94, 0.94, 0.94, 0.95, 0.96), temp = factor( rep(c(15, 25), each=20) ) )</p>
<p>head(germination) # obsT cumProp temp # 1 1 0.02 15 # 2 3 0.14 15 # 3 5 0.25 15 # 4 7 0.38 15 # 5 9 0.52 15 # 6 11 0.56 15</p>
<p>The traditional method of data analysis is based on: (1) transforming the observed counts into cumulative proportions of germinated seeds, and (2) fitting a Gaussian cumulative probability density function, using nonlinear least squares regression.</p>
<p>cumProp &lt;- as.numeric(unlist(with(germination, tapply(counts, temp, cumsum) ))/100)</p>
<p>mod &lt;- drm(cumProp ~ obsT, fct=LN.2(), curveid=temp, pmodels=list(~1, ~temp-1), data = germination) summary(mod) # Model fitted: Log-normal with lower limit at 0 and upper limit at 1 (2 parms) # # Parameter estimates: # # Estimate Std. Error t-value p-value<br />
# b:(Intercept) 0.871778 0.017726 49.182 &lt; 2.2e-16 <strong><em> # e:temp15 9.106622 0.178919 50.898 &lt; 2.2e-16 </em></strong> # e:temp25 5.429718 0.136514 39.774 &lt; 2.2e-16 *** # — # Signif. codes: 0 ‘<em><strong>’ 0.001 ‘</strong>’ 0.01 ‘</em>’ 0.05 ‘.’ 0.1 ‘’ 1 # # Residual standard error: # # 0.02043963 (37 degrees of freedom)</p>
<p>For this example, a survival regression model is more appropriate. The code does the following: (1) The dataset is reorganised, so that there is one record for each seed in the lot (200 records in all). For each temperature subset, records are added also for the seeds that did not germinate at the end of the assay. (2) Each record is characterised by three variables: the temperature at which it was tested and the two limits of the interval within which it germinated (‘timeBef’ and ‘timeAf’). Seeds which already germinated at the first monitoring time have ‘timeBef’ equal to NA (not available). Seeds which did not germinate at the final monitoring time have ‘timeAf’ equal to NA. (3) A survival model is fitted using a lognormal distribution of germination times (dist = “lognormal”).</p>
<p>counts &lt;- c(counts[1:20], 100 - sum(counts[1:20]), counts[21:40], 100 - sum(counts[21:40]))</p>
<p>germination2 &lt;- data.frame( temp = rep(factor(rep(c(15, 20), each=21)), counts), timeAf = rep(c(germination<span class="math inline">\(obsT[1:20], NA, germination\)</span>obsT[21:40], NA), counts), timeBef = rep(c(NA, germination<span class="math inline">\(obsT[1:20], NA, germination\)</span>obsT[21:40]), counts) ) rm(counts) head(germination2, 10)</p>
</div>
<div id="temp-timeaf-timebef" class="section level1">
<h1>temp timeAf timeBef</h1>
</div>
<div id="na" class="section level1">
<h1>1 15 1 NA</h1>
</div>
<div id="na-1" class="section level1">
<h1>2 15 1 NA</h1>
</div>
<div id="section-2" class="section level1">
<h1>3 15 3 1</h1>
</div>
<div id="section-3" class="section level1">
<h1>4 15 3 1</h1>
</div>
<div id="section-4" class="section level1">
<h1>5 15 3 1</h1>
</div>
<div id="section-5" class="section level1">
<h1>6 15 3 1</h1>
</div>
<div id="section-6" class="section level1">
<h1>7 15 3 1</h1>
</div>
<div id="section-7" class="section level1">
<h1>8 15 3 1</h1>
</div>
<div id="section-8" class="section level1">
<h1>9 15 3 1</h1>
</div>
<div id="section-9" class="section level1">
<h1>10 15 3 1</h1>
<p>library(survival) library(emmeans)</p>
<p>mod2 &lt;- survreg(Surv(timeBef, timeAf, type = “interval2”) ~ temp, dist = “lognormal”, data = germination2) summary(mod2)</p>
</div>
<div id="value-std.-error-z-p" class="section level1">
<h1>Value Std. Error z p</h1>
</div>
<div id="intercept-2.241-0.1173-19.11-2.11e-81" class="section level1">
<h1>(Intercept) 2.241 0.1173 19.11 2.11e-81</h1>
</div>
<div id="temp20--0.622-0.1666--3.73-1.89e-04" class="section level1">
<h1>temp20 -0.622 0.1666 -3.73 1.89e-04</h1>
</div>
<div id="logscale-0.145-0.0563-2.58-9.89e-03" class="section level1">
<h1>Log(scale) 0.145 0.0563 2.58 9.89e-03</h1>
</div>
<div id="section-10" class="section level1">
<h1></h1>
</div>
<div id="scale-1.16" class="section level1">
<h1>Scale= 1.16</h1>
</div>
<div id="section-11" class="section level1">
<h1></h1>
</div>
<div id="log-normal-distribution" class="section level1">
<h1>Log Normal distribution</h1>
</div>
<div id="loglikmodel--525.3-loglikintercept-only--532.1" class="section level1">
<h1>Loglik(model)= -525.3 Loglik(intercept only)= -532.1</h1>
</div>
<div id="chisq-13.57-on-1-degrees-of-freedom-p-0.00023" class="section level1">
<h1>Chisq= 13.57 on 1 degrees of freedom, p= 0.00023</h1>
</div>
<div id="number-of-newton-raphson-iterations-3" class="section level1">
<h1>Number of Newton-Raphson Iterations: 3</h1>
</div>
<div id="n-200" class="section level1">
<h1>n= 200</h1>
<p>emmeans(mod2, ~temp, transform=“response”) # temp response SE df lower.CL upper.CL # 15 9.407324 1.1034551 197 7.231223 11.583425 # 20 5.050110 0.5974451 197 3.871901 6.228319 # # Confidence level used: 0.95</p>
<p>SAS code Here as well, it would be nice to give the code to perform with SAS the very same analyses as before (read the data, make a nonlinear least squares regression and fit a survival regression model). . Or we simply skip this part</p>
<p>Example 3: potato starch grain assessed in classes</p>
<p>This dataset refers to an experiment which aimed to compare diameters of starch grains from tubers of two potato producers. Starch grains were sampled from tubers, collected from the production fields of the producers. The dataset shows the counts of starch grains assigned to one of five diameter classes (&lt;4, 4-8, 8-12, 12-16, &gt;16 µm). For each producer, the diameters were taken from twelve photos taken with a microscope.</p>
<p>R code The original dataset is in gouped form and consists of the counts of starch grains assigned to each of 5 diameter classes, as observed in 24 photos:</p>
<p>Class &lt;- matrix(c(21, 23, 17, 24, 19, 13, 34, 23, 21, 26, 29, 40, 37, 31, 23, 32, 32, 22, 29, 28, 28, 33, 20, 34, 42, 45, 40, 42, 16, 7, 16, 13, 28, 27, 25, 19, 33, 40, 37, 36, 47, 18, 20, 13, 12, 13, 13, 12, 40, 22, 33, 30, 15, 11, 8, 10, 26, 27, 21, 22, 30, 24, 31, 30, 28, 25, 27, 21, 16, 24, 13, 8, 29, 12, 18, 8, 8, 4, 5, 4, 8, 14, 16, 7, 27, 20, 13, 5, 20, 22, 14, 17, 13, 12, 20, 19, 14, 10, 8, 3, 3, 10, 9, 1, 8, 8, 7, 10, 19, 15, 19, 20, 20, 13, 18, 11, 21, 26, 22, 16), 24, 5) colnames(Class) &lt;- paste(“c”, 1:5, sep = “”) Group &lt;- rep(c(“P1”, “P2”), each = 12) Photo &lt;- rep(1:24) dataset &lt;- data.frame(Group = Group, Photo = Photo, Class) rm(Class, Group, Photo) head(dataset) # Group Photo c1 c2 c3 c4 c5 # 1 P1 1 21 42 40 29 14 # 2 P1 2 23 45 22 12 10 # 3 P1 3 17 40 33 18 8 # 4 P1 4 24 42 30 8 3 # 5 P1 5 19 16 15 8 3 # 6 P1 6 13 7 11 4 10</p>
<p>It is useful to reorganise the above dataset in ungrouped form, so that we have one line for each starch grain (2441 lines), containing all the information about each individual starch grain (i.e. the photo, the class and the limits of the classes (sizeLow and sizeUp). This is done by using the facilities provided in the reshape package (Wickham, 2007).</p>
<p>moltenData &lt;- melt(dataset, id=c(“Group”,“Photo” )) datasetR &lt;- moltenData[rep(seq_len(nrow(moltenData)), moltenData$value),][,1:3] names(datasetR)[3] &lt;- “Class” row.names(datasetR) &lt;- 1:2441 rm(moltenData)</p>
</div>
<div id="imputing-the-diameter-interval-for-each-starch-grain" class="section level1">
<h1>Imputing the diameter interval for each starch grain</h1>
<p>datasetR<span class="math inline">\(sizeLow = with(datasetR, ifelse(Class==&quot;c1&quot;, NA,  ifelse(Class==&quot;c2&quot;, 4,  ifelse(Class==&quot;c3&quot;, 8,  ifelse(Class==&quot;c4&quot;, 12, 16)))) ) datasetR\)</span>sizeUp = with(datasetR, ifelse(Class==“c1”, 4, ifelse(Class==“c2”, 8, ifelse(Class==“c3”, 12, ifelse(Class==“c4”, 16, NA)))) )</p>
<p># Group Photo Class sizeLow sizeUp # 1 P1 1 c1 NA 4 # 2 P1 1 c1 NA 4 # 3 P1 1 c1 NA 4 # 4 P1 1 c1 NA 4 # 5 P1 1 c1 NA 4 # 6 P1 1 c1 NA 4</p>
<p>The traditional method of analysis for this example would be to fit an unordered multinomial logit model. The following code</p>
<ol style="list-style-type: decimal">
<li>loads the dataset,</li>
<li>loads the nnet package (Venables and Ripley, 2002. The library emmeans is also necessary, but it has already been loaded)</li>
<li>fits the multinomial model,</li>
<li>fits a ‘null’ model, where there is no difference between producers, and</li>
<li>compares the full and reduced models, by using a likelihood ratio test.</li>
</ol>
<p>library(nnet) mmod &lt;- multinom(Class ~ Group, datasetR) lsmeans(mmod, ~Group:Class)</p>
<p>#Group Class prob SE df lower.CL upper.CL #P1 c1 0.26387376 0.013294604 8 0.23321635 0.2945312 #P2 c1 0.26005662 0.011974488 8 0.23244341 0.2876698 #P1 c2 0.29117490 0.013704010 8 0.25957340 0.3227764 #P2 c2 0.21907767 0.011290852 8 0.19304092 0.2451144 #P1 c3 0.24112576 0.012903510 8 0.21137022 0.2708813 #P2 c3 0.20640863 0.011048063 8 0.18093176 0.2318855 #P1 c4 0.12102033 0.009838293 8 0.09833319 0.1437075 #P2 c4 0.15052247 0.009761136 8 0.12801325 0.1730317 #P1 c5 0.08280524 0.008313059 8 0.06363529 0.1019752 #P2 c5 0.16393459 0.010105997 8 0.14063012 0.1872391 Confidence level used: 0.95</p>
<p>mmodNull &lt;- multinom(Class ~ 1, datasetR) anova(mmod, mmodNull) Likelihood ratio tests of Multinomial Models</p>
</div>
<div id="response-class" class="section level1">
<h1>Response: Class</h1>
</div>
<div id="model-resid.-df-resid.-dev-test-df-lr-stat.-prchi" class="section level1">
<h1>Model Resid. df Resid. Dev Test Df LR stat. Pr(Chi)</h1>
</div>
<div id="section-12" class="section level1">
<h1>1 1 9760 7651.197</h1>
</div>
<div id="group-9756-7599.133-1-vs-2-4-52.06482-1.337066e-10" class="section level1">
<h1>2 Group 9756 7599.133 1 vs 2 4 52.06482 1.337066e-10</h1>
<p>  The analyses may be more efficiently performed by fitting a survival regression method. The following R code fits this model using a Gaussian CDF (dist=‘gaussian’). The survival package is necessary.</p>
<p>surv.reg1 &lt;- survreg(Surv(sizeLow, sizeUp, type=“interval2”) ~ Group - 1, dist=“gaussian”, data=datasetR) summary(surv.reg1)</p>
</div>
<div id="call" class="section level1">
<h1>Call:</h1>
</div>
<div id="survregformula-survsizelow-sizeup-type-interval2" class="section level1">
<h1>survreg(formula = Surv(sizeLow, sizeUp, type = “interval2”) ~</h1>
</div>
<div id="group---1-data-datasetr-dist-gaussian" class="section level1">
<h1>Group - 1, data = datasetR, dist = “gaussian”)</h1>
</div>
<div id="value-std.-error-z-p-1" class="section level1">
<h1>Value Std. Error z p</h1>
</div>
<div id="groupp1-7.34-0.2122-34.6-3.46e-262" class="section level1">
<h1>GroupP1 7.34 0.2122 34.6 3.46e-262</h1>
</div>
<div id="groupp2-8.67-0.1920-45.1-0.00e00" class="section level1">
<h1>GroupP2 8.67 0.1920 45.1 0.00e+00</h1>
</div>
<div id="logscale-1.89-0.0206-91.7-0.00e00" class="section level1">
<h1>Log(scale) 1.89 0.0206 91.7 0.00e+00</h1>
</div>
<div id="section-13" class="section level1">
<h1></h1>
</div>
<div id="scale-6.63" class="section level1">
<h1>Scale= 6.63</h1>
</div>
<div id="section-14" class="section level1">
<h1></h1>
</div>
<div id="gaussian-distribution" class="section level1">
<h1>Gaussian distribution</h1>
</div>
<div id="loglikmodel--3824.4-loglikintercept-only--3835.2" class="section level1">
<h1>Loglik(model)= -3824.4 Loglik(intercept only)= -3835.2</h1>
</div>
<div id="chisq-21.57-on-1-degrees-of-freedom-p-3.4e-06" class="section level1">
<h1>Chisq= 21.57 on 1 degrees of freedom, p= 3.4e-06</h1>
</div>
<div id="number-of-newton-raphson-iterations-3-1" class="section level1">
<h1>Number of Newton-Raphson Iterations: 3</h1>
</div>
<div id="n-2441" class="section level1">
<h1>n= 2441</h1>
<p>SAS code Just wondering whether it might be useful to have SAS code for the multinomial and survival models, by using PROC LOGISTIC and PROC LIFEREG (maybe I’m wrong with the names).</p>
<p>SAS code here</p>
<p>With SAS, we can also use PROC NLMIXED. The following SAS code</p>
<ol style="list-style-type: decimal">
<li>loads the data</li>
<li>defines a correct likelihood, and</li>
<li>fits the model using NLMIXED</li>
</ol>
<p>data datasetSAS; input Photo Groups n1 n2 n3 n4 n5; n=sum(of n1-n5); datalines; 1 1 21 42 40 29 14 2 1 23 45 22 12 10 3 1 17 40 33 18 8 4 1 24 42 30 8 3 5 1 19 16 15 8 3 6 1 13 7 11 4 10 7 1 34 16 8 5 9 8 1 23 13 10 4 1 9 1 21 28 26 8 8 10 1 26 27 27 14 8 11 1 29 25 21 16 7 12 1 40 19 22 7 10 13 2 37 33 30 27 19 14 2 31 40 24 20 15 15 2 23 37 31 13 19 16 2 32 36 30 5 20 17 2 32 47 28 20 20 18 2 22 18 25 22 13 19 2 29 20 27 14 18 20 2 28 13 21 17 11 21 2 28 12 16 13 21 22 2 33 13 24 12 26 23 2 20 13 13 20 22 24 2 34 12 8 19 16 ;</p>
<p>proc nlmixed data=datasetSAS technique=nrridg; parms sigma=4 mu1=8 mu2=10; t1=4; t2=8; t3=12; t4=16; if groups=1 then mu=mu1; else mu=mu2; pi1=probnorm((t1-mu)/sigma); pi2=probnorm((t2-mu)/sigma) - probnorm((t1-mu)/sigma); pi3=probnorm((t3-mu)/sigma) - probnorm((t2-mu)/sigma); pi4=probnorm((t4-mu)/sigma) - probnorm((t3-mu)/sigma); pi5=1-probnorm((t4-mu)/sigma); logL=lgamma(n+1)-lgamma(n1+1)-lgamma(n2+1)-lgamma(n3+1)-lgamma(n4+1)-lgamma(n5+1) + n1<em>log(pi1)+n2</em>log(pi2)+n3<em>log(pi3)+n4</em>log(pi4)+n5<em>log(pi5); model n1 ~ general(logL); estimate ‘diff’ mu1-mu2; estimate ‘sigma2’ sigma</em>sigma; run;</p>
<pre><code>                         The NLMIXED Procedure

             NOTE: GCONV convergence criterion satisfied.


                             Fit Statistics

                -2 Log Likelihood                  671.9
                AIC (smaller is better)            677.9
                AICC (smaller is better)           679.1
                BIC (smaller is better)            681.4

                           Parameter Estimates

           Standard                               95% Confidence</code></pre>
<p>Par. Estimate Error DF t Value Pr &gt; |t| Limits Gradient</p>
<p>sigma 6.6256 0.1367 24 48.48 &lt;.0001 6.3435 6.9076 4.6E-10 mu1 7.3398 0.2122 24 34.59 &lt;.0001 6.9019 7.7778 5.531E-9 mu2 8.6666 0.1920 24 45.13 &lt;.0001 8.2703 9.0629 8.598E-9</p>
<pre><code>                          Additional Estimates

                St.</code></pre>
<p>Label Estimate Error DF t Value Pr &gt; |t| Alpha Lower Upper</p>
<p>diff -1.3267 0.2853 24 -4.65 0.0001 0.05 -1.9156 -0.7379 sigma2 43.8985 1.8109 24 24.24 &lt;.0001 0.05 40.1609 47.6360</p>
<p>Example 3: estimation of random effects SAS code The following SAS code: 1. defines a correct conditional likelihood, and 2. fits the random-effects model by Gaussian quadrature using NLMIXED</p>
<p>options linesize=100; proc nlmixed data=datasetSAS technique=nrridg; parms sigma=4 mu1=8 mu2=10 sigma2p=1.6; t1=4; t2=8; t3=12; t4=16; if groups=1 then mu=mu1; else mu=mu2; random p ~ normal(0,sigma2p) subject=photo; mu=mu+p; pi1=probnorm((t1-mu)/sigma); pi2=probnorm((t2-mu)/sigma) - probnorm((t1-mu)/sigma); pi3=probnorm((t3-mu)/sigma) - probnorm((t2-mu)/sigma); pi4=probnorm((t4-mu)/sigma) - probnorm((t3-mu)/sigma); pi5=1-probnorm((t4-mu)/sigma); logL=lgamma(n+1)-lgamma(n1+1)-lgamma(n2+1)-lgamma(n3+1)-lgamma(n4+1)-lgamma(n5+1) + n1<em>log(pi1)+n2</em>log(pi2)+n3<em>log(pi3)+n4</em>log(pi4)+n5<em>log(pi5); model n1 ~ general(logL); estimate ‘diff’ mu1-mu2; estimate ‘sigmap’ sqrt(sigma2p); estimate ‘sigma2’ sigma</em>sigma; run; The NLMIXED Procedure</p>
<pre><code>                   NOTE: GCONV convergence criterion satisfied.


                                 Fit Statistics

                    -2 Log Likelihood                  666.2
                    AIC (smaller is better)            674.2
                    AICC (smaller is better)           676.3
                    BIC (smaller is better)            678.9

                               Parameter Estimates

           Stand.                      95% Confidence</code></pre>
<p>Param. Est. Error DF t Value Pr&gt;|t| Limits Gradient</p>
<p>sigma 6.5931 0.1364 23 48.33 &lt;.0001 6.3108 6.8753 3.18E-11 mu1 7.2367 0.2918 23 24.80 &lt;.0001 6.6330 7.8403 2.07E-10 mu2 8.6992 0.2724 23 31.93 &lt;.0001 8.1357 9.2627 2.79E-10 sigma2p 0.4424 0.2862 23 1.55 0.1358 -0.1496 1.0345 2.53E-1</p>
<pre><code>                               Additional Estimates

              Stand.</code></pre>
<p>Label Estimate Error DF t Value Pr&gt;|t| Alpha Lower Upper</p>
<p>diff -1.4625 0.3995 23 -3.66 0.0013 0.05 -2.2889 -0.6362 sigmap 0.6652 0.2151 23 3.09 0.0051 0.05 0.2201 1.1102 sigma2 43.4685 1.7990 23 24.16 &lt;.0001 0.05 39.7470 47.1900</p>
<p>  Example 3: the Bayesian perspective R code The first step is to specify an appropriate model (in JAGS code), which requires the following steps: 1. a value of zero is assigned to all observations (the “zeros trick”, as explained in the text); 2. Equation (6) is used, in which the expected diameter is assigned to all N observations, by using the for() loop; 3. the likelihood function is calculated for the observations in the first class (left-censored; Equation 3), assumed to be in the first N1 rows of the database; 4. the likelihood function is calculated for the observations in the 2nd to 4th class (interval-censored; Equation 4), assumed to be in the rows from N1+1 to N2; 5. the likelihood function is calculated for the observations in the 5th class (right-censored; Equation 5), assumed to be in the rows from N2+1 to N; and 6. the prior distributions are specified for all the parameters.</p>
</div>
<div id="save-bugs-description-of-the-model-to-working-directory" class="section level1">
<h1>Save BUGS description of the model to working directory</h1>
<p>modelSpec &lt;- &quot; data{ for (i in 1:N) { zeros[i] &lt;- 0 }}</p>
<p>model{ for (i in 1:N) { #Equation 6 exp[i] &lt;- mu[Group[i]] + gamma[Photo[i]] }</p>
<p>for (i in 1:N1) { #Likelihood for left-censored S2[i] &lt;- pnorm(high[i], exp[i], tau.e) L[i] &lt;- S2[i] #(Equation 3)<br />
phi[i] &lt;- -(log(L[i])) zeros[i] ~ dpois(phi[i])<br />
}</p>
<p>for (i in (N1+1):N2) { #Likelihood for interval-censored S[i] &lt;- pnorm(low[i], exp[i], tau.e) S2[i] &lt;- pnorm(high[i], exp[i], tau.e) L[i] &lt;- S2[i] - S[i] #(Equation 4)<br />
phi[i] &lt;- -(log(L[i])) zeros[i] ~ dpois(phi[i])<br />
}</p>
<p>for (i in (N2+1):N) { #Likelihood for right-censored S[i] &lt;- pnorm(low[i], exp[i], tau.e) L[i] &lt;- 1 - S[i] #(Equation 5)<br />
phi[i] &lt;- -(log(L[i])) zeros[i] ~ dpois(phi[i])<br />
}</p>
</div>
<div id="priors" class="section level1">
<h1>Priors</h1>
<p>sigma.e ~ dunif(0, 100) sigma.P ~ dunif(0, 100) for(i in 1:2){ mu[i] ~ dnorm(0, 0.000001) }for(i in 1:24){ gamma[i] ~ dnorm(0, tau.P) }</p>
</div>
<div id="derived-quantities" class="section level1">
<h1>Derived quantities</h1>
<p>sigma2p &lt;- sigma.P<em>sigma.P sigma2e &lt;- sigma.e</em>sigma.e tau.P &lt;- 1 / sigma2p tau.e &lt;- 1 / sigma2e diff &lt;- mu[1] - mu[2] }</p>
<p>&quot;</p>
<p>The above model is coded within R and assigned to a string of text (“modelSpec”). Using R to fit the above model requires 1. loading the rjags library (Plummer, 2016) 2. storing the model definition to an external text file (“censoredMixedModel.txt”), using the function writeLines(); 3. sorting the dataset in an ascending class order so that the individuals in the first diameter class begin the data set (from row 1 to row 639) and the individuals in the 5th diameter class end it (from row 2,131 to row 2,441); 4. creating two lists: a list of all the data needed for the analysis (win.data) and a list of the initial values for the parameters to be estimated (init); and 5. sending the model specification and the other data to JAGS, using the function jags.model(), provided by the package rjags; this function returns samples from the posterior distribution for all the estimated parameters.</p>
<p>library(rjags) writeLines(modelSpec, con=“censoredMixedModel.txt”) dataset_jags &lt;- datasetR[order(datasetR$Class), ] N1 &lt;- 639; N2 &lt;- 2130; N &lt;- 2441 win.data &lt;- list(low = dataset_jags<span class="math inline">\(sizeLow, high=dataset_jags\)</span>sizeUp, N1=N1, N2=N2, N=N, Group=factor(dataset_jags<span class="math inline">\(Group),  Photo=factor(dataset_jags\)</span>Photo))</p>
<p>init &lt;- list (mu = c(7.3, 8.7), sigma.e = 1.7, sigma.P=0.5) mcmc &lt;- jags.model(“censoredMixedModel.txt”,data = win.data, inits = init, n.chains = 4, n.adapt = 100) params &lt;- c(“mu”, “sigma.e”, “sigma.P”, “sigma2p”, “sigma2e”, “diff”) res3 &lt;- coda.samples(mcmc, params, n.iter=10000) burn.in &lt;- 1000 summary(window(res3, start=burn.in))</p>
</div>
<div id="output-is-shown-in-table-7-in-the-main-paper" class="section level1">
<h1>[Output is shown in Table 7 in the main paper]</h1>
<p>SAS code Should we show the code for PROC MCMC? Example 3: fitting a generalized linear model (GLM) for interval-censored data SAS code We can exploit the fact that the kernels of the multinomial and Poisson likelihoods are identical. Thus, we can model the observed counts as Poisson random variables with expectation (McCullagh and Nelder 1989; §6.4.2; Keen and Engel, 1997). The cutpoints are known. If the variance were known as well, we would have had a regular GLM with link functions given by the Equation (4) to (6) in the main paper. We can use the ML estimate of the variance; below, we try this in GLIMMIX—and it works perfectly.</p>
<p>/<em>can we fit threshold model using the known thresholds?</em>/ data StarchGrainsSAS_Counts; set datasetSAS; count=n1; cat=1; output; count=n2; cat=2; output; count=n3; cat=3; output; count=n4; cat=4; output; count=n5; cat=5; output; run;</p>
<p>/<em>yes we can if the variance is known!</em>/ proc glimmix data=StarchGrainsSAS_Counts method=quad; class groups photo; sigma=6.5931; t1=4; t2=8; t3=12; t4=16; if cat=1 then <em>mu</em>=n<em>probnorm((t1-<em>linp</em>)/sigma); if cat=2 then <em>mu</em>=n</em>( probnorm((t2-<em>linp</em>)/sigma) - probnorm((t1-<em>linp</em>)/sigma) ); if cat=3 then <em>mu</em>=n<em>( probnorm((t3-<em>linp</em>)/sigma) - probnorm((t2-<em>linp</em>)/sigma) ); if cat=4 then <em>mu</em>=n</em>( probnorm((t4-<em>linp</em>)/sigma) - probnorm((t3-<em>linp</em>)/sigma) ); if cat=5 then <em>mu</em>=n*(1-probnorm((t4-<em>linp</em>)/sigma)); model count=groups/dist=poisson solution; random int/sub=photo; run; Fit Statistics</p>
<pre><code>                            -2 Log Likelihood             820.31
                            AIC  (smaller is better)      826.31
                            AICC (smaller is better)      826.51
                            BIC  (smaller is better)      829.84
                            CAIC (smaller is better)      832.84
                            HQIC (smaller is better)      827.24


                               Fit Statistics for Conditional
                                        Distribution

                          -2 log L(count | r. effects)      793.55
                          Pearson Chi-Square                223.08
                          Pearson Chi-Square / DF             1.86


                               Covariance Parameter Estimates

                                                            Standard
                        Cov Parm     Subject    Estimate       Error

                        Intercept    Photo        0.4424      0.2862


                                Solutions for Fixed Effects

                                           Standard</code></pre>
<p>Effect Groups Estimate Error DF t Value Pr &gt; |t|</p>
<p>Intercept 8.6992 0.2721 22 31.97 &lt;.0001 Groups 1 -1.4625 0.3994 96 -3.66 0.0004 Groups 2 0 . . . .</p>
<p>This agrees almost exactly with the corresponding output from NLMIXED (apart from the “residual” degrees of freedom). The only problem is how to estimate the variance. A simple option is a grid search for the optimal value of . Another idea is to use Pregibon’s (1980) approach for models with parameters in the link function (also see McCullagh and Nelder, 1989; § 11.3), but this is tricky because the link function for the Poisson approach looks messy. Here is the solution within a GLMM framework. If we blow up the data to the level of individual grains, we can fit a random grain effect in order to properly model and estimate the residual variance. At an individual grain level (that is, for a particular grain), we have and the observed count is zero for all categories except for the category the grain falls into; for this category, the count equals one. If we use a probit link with unit variance, the total residual variance will be parameterized as . This works whenever σ² ≥ 1 (in this example ). If , we need to use a scale parameter in the probit link. Our model can then be stated as follows:</p>
<p>, (7) (8) (9) (10) (11)</p>
<p>Then, conditionally on the random effect , the multinomial probabilities are , (12) for , and (13) . (14)</p>
<p>To implement the above idea, we need to supply the user-defined link functions (12) to (14) and play with the scaling factor so that . In our example, this holds for .</p>
<p>/<em>Another idea: Blow up the data to individual grains and fit additional random effect for units</em>/</p>
<p>data StarchGrainsSAS_Individual; set StarchGrainsSAS_Counts; n=1; do grain=1 to count; do cat2=1 to 5; if cat2=cat then count2=1; else count2=0; output; end; end; run;</p>
<p>proc glimmix data=StarchGrainsSAS_Individual method=quad; class groups photo grain cat; theta=4; t1=4; t2=8; t3=12; t4=16; if cat2=1 then <em>mu</em>=n<em>probnorm((t1-<em>linp</em>)/theta); if cat2=2 then <em>mu</em>=n</em>( probnorm((t2-<em>linp</em>)/theta) - probnorm((t1-<em>linp</em>)/theta) ); if cat2=3 then <em>mu</em>=n<em>( probnorm((t3-<em>linp</em>)/theta) - probnorm((t2-<em>linp</em>)/theta) ); if cat2=4 then <em>mu</em>=n</em>( probnorm((t4-<em>linp</em>)/theta) - probnorm((t3-<em>linp</em>)/theta) ); if cat2=5 then <em>mu</em>=n<em>(1-probnorm((t4-<em>linp</em>)/theta)); model count2=groups/dist=poisson solution; random int/sub=photo</em>grain*cat; run;</p>
<pre><code>                    Convergence criterion (GCONV=1E-8) satisfied.


                                    Fit Statistics

                         -2 Log Likelihood           12531.05
                         AIC  (smaller is better)    12537.05
                         AICC (smaller is better)    12537.05
                         BIC  (smaller is better)    12554.45
                         CAIC (smaller is better)    12557.45
                         HQIC (smaller is better)    12543.37


                            Fit Statistics for Conditional
                                     Distribution

                       -2 log L(count2 | r. effects)     9173.19
                       Pearson Chi-Square                3549.85
                       Pearson Chi-Square / DF              0.29


                            Covariance Parameter Estimates

                                                             Standard
                 Cov Parm     Subject            Estimate       Error

                 Intercept    Photo*grain*cat     27.8470      1.8031


                              Solutions for Fixed Effects

                                         Standard</code></pre>
<p>Effect Groups Estimate Error DF t Value Pr &gt; |t|</p>
<p>Intercept 8.6670 0.1919 2439 45.16 &lt;.0001 Groups 1 -1.3264 0.2851 9764 -4.65 &lt;.0001</p>
<pre><code>                                 The GLIMMIX Procedure

                              Solutions for Fixed Effects

                                         Standard</code></pre>
<p>Effect Groups Estimate Error DF t Value Pr &gt; |t| Groups 2 0 . . . .</p>
<pre><code>                            Type III Tests of Fixed Effects

                                  Num      Den
                    Effect         DF       DF    F Value    Pr &gt; F
                    Groups          1     9764      21.64    &lt;.0001</code></pre>
<p>The above result agrees with the full ML estimates obtained with NLMIXED. Adding a random photo effect, we obtain the following result:</p>
<p>/<em>add random photo effect</em>/ proc glimmix data=StarchGrainsSAS_Individual method=laplace; class groups photo grain cat; theta=4; t1=4; t2=8; t3=12; t4=16; if cat2=1 then <em>mu</em>=n<em>probnorm((t1-<em>linp</em>)/theta); if cat2=2 then <em>mu</em>=n</em>( probnorm((t2-<em>linp</em>)/theta) - probnorm((t1-<em>linp</em>)/theta) ); if cat2=3 then <em>mu</em>=n<em>( probnorm((t3-<em>linp</em>)/theta) - probnorm((t2-<em>linp</em>)/theta) ); if cat2=4 then <em>mu</em>=n</em>( probnorm((t4-<em>linp</em>)/theta) - probnorm((t3-<em>linp</em>)/theta) ); if cat2=5 then <em>mu</em>=n<em>(1-probnorm((t4-<em>linp</em>)/theta)); model count2=groups/dist=poisson solution; random int/sub=photo; random grain</em>cat/sub=photo; run;</p>
<pre><code>                    Convergence criterion (GCONV=1E-8) satisfied.


                                    Fit Statistics

                         -2 Log Likelihood           12550.22
                         AIC  (smaller is better)    12558.22
                         AICC (smaller is better)    12558.22
                         BIC  (smaller is better)    12562.93
                         CAIC (smaller is better)    12566.93
                         HQIC (smaller is better)    12559.47


                            Fit Statistics for Conditional
                                     Distribution

                       -2 log L(count2 | r. effects)     9269.27
                       Pearson Chi-Square                3659.44
                       Pearson Chi-Square / DF              0.30


                            Covariance Parameter Estimates

                                                         Standard
                     Cov Parm     Subject    Estimate       Error

                     Intercept    Photo        0.4184      0.2710
                     grain*cat    Photo       25.3592      1.6184


                                 The GLIMMIX Procedure

                              Solutions for Fixed Effects

                                         Standard</code></pre>
<p>Effect Groups Estimate Error DF t Value Pr &gt; |t| Intercept 8.7166 0.2653 22 32.85 &lt;.0001 Groups 1 -1.4475 0.3892 9764 -3.72 0.0002 Groups 2 0 . . . .</p>
<pre><code>                            Type III Tests of Fixed Effects

                                  Num      Den
                    Effect         DF       DF    F Value    Pr &gt; F
                    Groups          1     9764      13.83    0.0002</code></pre>
<p>To get this to run, we had to use the Laplace method instead of adaptive Gaussian quadrature with three quadrature points; the computer, unfortunately, went out of memory. It is likely due to the smaller accuracy of the Laplace approximation this approach uses that the results do not agree so well with those of NLMIXED. So, this approach works and has all the virtues of using a GLMM framework, but it is computationally more demanding than the direct approach using NLMIXED.</p>
<p>References H. Wickham. Reshaping data with the reshape package. Journal of Statistical Software, 21(12), 2007. Russell Lenth (2018). emmeans: Estimated Marginal Means, aka Least-Squares Means. R package version 1.2.3. <a href="https://CRAN.R-project.org/package=emmeans" class="uri">https://CRAN.R-project.org/package=emmeans</a> Terry M. Therneau, Patricia M. Grambsch (2000). <em>Modeling Survival Data: Extending the Cox Model</em>. Springer, New York. ISBN 0-387-98784-3.</p>
<p>Venables, W. N. &amp; Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth Edition. Springer, New York. ISBN 0-387-95457-0</p>
<p>Martyn Plummer (2016). rjags: Bayesian Graphical Models using MCMC. R package version 4-6. <a href="https://CRAN.R-project.org/package=rjags" class="uri">https://CRAN.R-project.org/package=rjags</a></p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
